{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affiliated-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal of this notebook is to develop the framework for a reinforcement learning algorithm to drive\n",
    "#the optimization of performance characteristics of the athlete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "theoretical-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The general idea is to drive the athlete to faster and faster times while keeping the heart rate of the training as \n",
    "#low as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-smell",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-fraction",
   "metadata": {},
   "source": [
    "The idea of a reinforcement learning model is that an algorithm is incentivized to choose an action that will return the most reward. The actions an algorithm takes and the history of the states that those actions lead to are recorded and used to inform future decisions. \n",
    "An algorithm also should use some randomization to ensure that it can \"test out\" sub optimal actions in the short time, for long term gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-quantity",
   "metadata": {},
   "source": [
    "In this way, I plan to build the algorithm that will \"learn\" the most effective way to train an individual towards a known goal, by varying the \"actions\" that it takes (input of the workouts) and monitoring the effectiveness of those actions (performance based metrics of the workouts completed)\n",
    "\n",
    "Therefore I will need to define the reward function as some function of the workout variables, and success and failure based on the balance of one or more variables. For example a run's distance might not be the only success variable, we might also be interested in the pace of that run, and the heart rate required to accomplish said run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-forestry",
   "metadata": {},
   "source": [
    "Some rules will have to be in place to prevent \"wild\" experimentation, for example a runner who has never run more than 5 miles on the program should not be given a workout to run 20+ miles in one go in a week, as a way to optimize for a variable for example.\n",
    "\n",
    "Rewards should also most likely be implemented for consistency of engagement, as that is also crucial to the athlete using the program that the algorithm is running behind. If athletes decide to not use the app, then it is most likely not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-taxation",
   "metadata": {},
   "source": [
    "This notebook will attempt to:\n",
    "* Define the basic classes of the objects we will need, pythonically\n",
    "* Define the functions to pull up sufficient data to predict a training plan for a week for an athlete\n",
    "* Use the greedy reinforcement learning model to optimize future workouts for better returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-delicious",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-damages",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
