{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accurate-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal of this notebook is to develop the framework for a reinforcement learning algorithm to drive\n",
    "#the optimization of performance characteristics of the athlete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sized-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The general idea is to drive the athlete to faster and faster times while keeping the heart rate of the training as \n",
    "#low as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-change",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-strength",
   "metadata": {},
   "source": [
    "The idea of a reinforcement learning model is that an algorithm is incentivized to choose an action that will return the most reward. The actions an algorithm takes and the history of the states that those actions lead to are recorded and used to inform future decisions. \n",
    "An algorithm also should use some randomization to ensure that it can \"test out\" sub optimal actions in the short time, for long term gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-preliminary",
   "metadata": {},
   "source": [
    "In this way, I plan to build the algorithm that will \"learn\" the most effective way to train an individual towards a known goal, by varying the \"actions\" that it takes (input of the workouts) and monitoring the effectiveness of those actions (performance based metrics of the workouts completed)\n",
    "\n",
    "Therefore I will need to define the reward function as some function of the workout variables, and success and failure based on the balance of one or more variables. For example a run's distance might not be the only success variable, we might also be interested in the pace of that run, and the heart rate required to accomplish said run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-extraction",
   "metadata": {},
   "source": [
    "Some rules will have to be in place to prevent \"wild\" experimentation, for example a runner who has never run more than 5 miles on the program should not be given a workout to run 20+ miles in one go in a week, as a way to optimize for a variable for example.\n",
    "\n",
    "Rewards should also most likely be implemented for consistency of engagement, as that is also crucial to the athlete using the program that the algorithm is running behind. If athletes decide to not use the app, then it is most likely not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-faculty",
   "metadata": {},
   "source": [
    "This notebook will attempt to:\n",
    "* Define the basic classes of the objects we will need, pythonically\n",
    "* Define the functions to pull up sufficient data to predict a training plan for a week for an athlete\n",
    "* Use the greedy reinforcement learning model to optimize future workouts for better returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-seminar",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "humanitarian-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, insert, MetaData\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-miniature",
   "metadata": {},
   "source": [
    "First let's do some local development of classes and functions, and test them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fancy-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Athlete():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elect-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPlan():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chicken-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Workout():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-wells",
   "metadata": {},
   "source": [
    "Two important metrics that we will be using for goal seeking in the reinforcement algorithm will have to do with athlete performance. The faster an athlete runs for each heartbeat as a ratio relative to threshold heartrate will be the metric by which running performance will be measured, and the maximizing of this variable will be the goal we shall seek.\n",
    "\n",
    "Similarly, the higher the wattage supplied on the bicycle per heart beat as a ratio relative to biking threshold heartrate will be the metric by which running performance will be measured, and maximizing that variable will be the goal we shall seek.\n",
    "\n",
    "Essentially, we are trying to optimize the workout schedule that will lead to the highest run pace and/or highest wattage for the % of threshold that the athlete's heart averages for that workout, specifically a weekly benchmark usually the long run or long ride."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-marketing",
   "metadata": {},
   "source": [
    "Functionally, the creation of the workouts can be automated and done at a slow rate, in off peak hours (possibly at midnight). To give the athlete some idea of the upcoming workouts and structure, a placeholder workout will be placed in the calendar and then fleshed out the night before based on the performance of past workouts.\n",
    "\n",
    "So that the workout is timed and adjusted exactly to the performance of the athlete in recent days, the athlete entry in the database most likely needs to store data about what type of workouts work well for them so that recommendations are optimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-reduction",
   "metadata": {},
   "source": [
    "Different types of workout affinities can be recorded:\n",
    "* Propensity to complete a workout\n",
    "* Propensity for interval work\n",
    "* Propensity for duration\n",
    "* Preferred interval duration\n",
    "* Max 30s power or pace or hr\n",
    "* Max 2 min power or pace or hr\n",
    "* Max 5 min power or pace or hr\n",
    "* Max 10 min power or pace or hr\n",
    "* Max 20 min power or pace or hr\n",
    "* Max 40 min power or pace or hr\n",
    "* Avg workout power or pace or hr\n",
    "* Propensity for workout variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-bankruptcy",
   "metadata": {},
   "source": [
    "So, we will need to define a rewards matrix, and an actions matrix for each athlete. This should be a database entry that is captured and recorded and adjusted each time a workout is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-hepatitis",
   "metadata": {},
   "source": [
    "We will also need a decider, that picks the best action based on the responses of the reward matrix, thus making it 'greedy'. Occasionally we will probably want the decider to choose a random or novel approach, to ensure that the first choice taken doesn't overrule a different, more fruitful strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class training_generator(setup_params):\n",
    "    '''\n",
    "    prop_workout is the propensity to complete a workout, 100% similarity, as a float (0.0-1.0)\n",
    "    prop_int is the propensity for interval workouts, as a float (0.0-1.0)\n",
    "    prop_dur is the propensity for duration over intensity, time spent below .85 of threshold vs above .85, float (-1.0 to 1.0)\n",
    "    prop_int_dur is preferred interval duration, as a % of total workout time, as a float (0.0 - 1.0)\n",
    "    iters is the number of iterations to run training generation \n",
    "    reward_matrix is the matrix of rewards that the training generator receives when it chooses an action\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, prop_workout, prop_int, prop_dur, prop_int_dur, iters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
